# ---------------------------------------
# PSO for Data Clustering
# ---------------------------------------
# Step 1: Import Libraries
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import pairwise_distances_argmin
import pyswarms as ps

# Step 2: Load Dataset
data = pd.read_csv("/content/SCOA_A7.csv")
X = data[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']].values

# Step 3: Normalize Data
scaler = MinMaxScaler()
X = scaler.fit_transform(X)

# Step 4: Define Clustering Function
def cluster_cost(centroids_flat):
    n_clusters = 3
    centroids = centroids_flat.reshape(n_clusters, X.shape[1])
    labels = pairwise_distances_argmin(X, centroids)
    # Sum of squared distances (SSE)
    cost = sum(np.sum((X[labels == i] - centroids[i])**2) for i in range(n_clusters))
    return cost

# Wrap for all particles
def cost_wrapper(particles):
    return np.array([cluster_cost(p) for p in particles])

# Step 5: Run PSO Optimization

n_clusters = 3
dims = n_clusters * X.shape[1]
options = {'c1': 1.5, 'c2': 1.5, 'w': 0.5}

optimizer = ps.single.GlobalBestPSO(n_particles=20, dimensions=dims, options=options)
best_cost, best_pos = optimizer.optimize(cost_wrapper, iters=50)

# Step 6: Final Clusters
best_centroids = best_pos.reshape(n_clusters, X.shape[1])
labels = pairwise_distances_argmin(X, best_centroids)

# Step 7: Display Results
print("Best Centroids:\n", best_centroids)
print("Best Cost (SSE):", best_cost)
print("Cluster labels sample:", labels[:10])






ğŸ§  Theory Recap â€” What is PSO?
Particle Swarm Optimization (PSO)
is an evolutionary algorithm inspired by how birds flock or fish school.
Each â€œparticleâ€ (like a bird) represents one possible solution â€” here, a set of cluster centers.
All particles â€œflyâ€ through the search space, adjusting their positions based on:
	their own best position (cognitive behavior)
	the swarmâ€™s best position (social behavior)
Goal: minimize the cost function (in clustering, the total distance between points and their cluster centers).
________________________________________
âš™ï¸ Mathematical Formulas
For each particle i, at each iteration t:
1ï¸âƒ£ Velocity Update
v_i (t+1)=wâ‹…v_i (t)+c_1â‹…r_1â‹…(p_i^best-x_i (t))+c_2â‹…r_2â‹…(g^best-x_i (t))

	v_i (t): current velocity
	w: inertia weight (controls momentum / exploration)
	c_1: cognitive constant (self-attraction)
	c_2: social constant (swarm attraction)
	r_1,r_2: random numbers in [0,1]
	p_i^best: personal best position
	g^best: global best position
________________________________________
2ï¸âƒ£ Position Update
x_i (t+1)=x_i (t)+v_i (t+1)

	x_i (t): current position (i.e., centroids)
	v_i (t+1): updated velocity
________________________________________
3ï¸âƒ£ Fitness / Objective Function
For clustering, the cost is:
J=âˆ‘_(k=1)^K âˆ‘_(x_jâˆˆC_k)âˆ¥x_j-Î¼_k âˆ¥^2

	K: number of clusters
	x_j: data points
	Î¼_k: centroid of cluster k
	C_k: set of points in cluster k
This is the same as SSE (Sum of Squared Errors) â€” smaller is better.
________________________________________
ğŸ” Code Explanation
ğŸ§© Step 1: Import Libraries
import numpy as np, pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import pairwise_distances_argmin
import pyswarms as ps
	numpy, pandas â†’ for data handling
	MinMaxScaler â†’ normalize data between [0,1]
	pairwise_distances_argmin â†’ assigns each point to nearest cluster center
	pyswarms â†’ PSO library
________________________________________
ğŸ“Š Step 2: Load Dataset
data = pd.read_csv("/content/SCOA_A7.csv")
X = data[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']].values
We use only numeric columns for clustering.
.values â†’ converts DataFrame â†’ NumPy array.
________________________________________
âš–ï¸ Step 3: Normalize Data
scaler = MinMaxScaler()
X = scaler.fit_transform(X)
Normalization makes all features within the same range [0,1], preventing one column (like income) from dominating distance calculation.
________________________________________
ğŸ§® Step 4: Define Clustering Function
def cluster_cost(centroids_flat):
    n_clusters = 3
    centroids = centroids_flat.reshape(n_clusters, X.shape[1])
    labels = pairwise_distances_argmin(X, centroids)
    cost = sum(np.sum((X[labels == i] - centroids[i])**2) for i in range(n_clusters))
    return cost
	centroids_flat â†’ one particle (flattened centroid positions)
	Reshape â†’ convert to proper (3 Ã— features)
	Assign each data point to nearest centroid
	Compute SSE (sum of squared errors)
This function returns cost (fitness).
________________________________________
ğŸ§° Step 5: Run PSO Optimization
n_clusters = 3
dims = n_clusters * X.shape[1]
options = {'c1': 1.5, 'c2': 1.5, 'w': 0.5}

optimizer = ps.single.GlobalBestPSO(n_particles=20, dimensions=dims, options=options)
best_cost, best_pos = optimizer.optimize(cost_wrapper, iters=50)
	n_particles = 20 â†’ number of possible centroid sets
	dimensions = 3 clusters Ã— 3 features = 9 values per particle
	PSO will move centroids around to minimize the SSE
	c1, c2, w â†’ control balance between exploration and exploitation
________________________________________
ğŸ¯ Step 6: Extract Best Clusters
best_centroids = best_pos.reshape(n_clusters, X.shape[1])
labels = pairwise_distances_argmin(X, best_centroids)
Once optimization is done, reshape the best position to 3 centroids and assign each point to nearest cluster.
________________________________________
ğŸ“ˆ Step 7: Print Results
print("Best Centroids:\n", best_centroids)
print("Best Cost (SSE):", best_cost)
print("Cluster labels sample:", labels[:10])
This displays:
	Optimal cluster centers
	Best SSE (lower = better)
	Sample of cluster assignments
________________________________________
ğŸ” Summary of PSO Steps in This Code
Step	Description	What Happens Here
1	Initialize swarm	Random centroids (particles)
2	Evaluate fitness	Calculate SSE for each particle
3	Update velocity & position	Move centroids toward better solutions
4	Update bests	Track best individual and global positions
5	Repeat	Until iteration limit (50)
6	Output	Best cluster centroids and cost
________________________________________
ğŸ§¾ Conclusion
âœ… PSO mimics swarm intelligence to find optimal cluster centers.
âœ… It avoids local minima (unlike k-means).
âœ… This code shows a short, learnable, and working PSO example for clustering.

